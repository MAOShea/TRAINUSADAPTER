#!/usr/bin/env python3
"""
Evaluate complete training data size including system prompts, user prompts, and widget code.
Accounts for the full JSON structure that will be generated by create_dataset.py.
Generates actionable strategy recommendations (training_data_strategy.json) for create_dataset.py.
"""

import json
import os
import csv
import glob
import sys
import uuid
from pathlib import Path

# Rough token estimation: ~4 chars per token for code/text
CHARS_PER_TOKEN = 4
DEFAULT_MAX_SEQUENCE_LENGTH = 4095

# Import shared configuration from training_config module
from training_config import systemPrompt as SYSTEM_PROMPT, TOOL_DEFINITION

def estimate_tokens(text_length, chars_per_token=CHARS_PER_TOKEN):
    """Estimate token count from character length"""
    return int(text_length / chars_per_token)

def get_widget_code(widget_folder, downloads_dir='downloads'):
    """Read all JSX files for a widget and concatenate them"""
    widget_path = os.path.join(downloads_dir, widget_folder)
    
    if not os.path.exists(widget_path):
        return None
    
    jsx_files = glob.glob(os.path.join(widget_path, '**/*.jsx'), recursive=True)
    
    code_parts = []
    for jsx_file in sorted(jsx_files):
        try:
            with open(jsx_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    relative_path = os.path.relpath(jsx_file, widget_path)
                    code_parts.append(f"// {relative_path}\n{content}")
        except Exception as e:
            print(f"Warning: Could not read {jsx_file}: {e}", file=sys.stderr)
    
    return '\n\n'.join(code_parts) if code_parts else None

def get_user_prompt(widget_id, prompts_dir='prompts'):
    """Read user prompt from prompts folder"""
    prompt_file = os.path.join(prompts_dir, f'{widget_id}.prompt')
    
    if not os.path.exists(prompt_file):
        return None
    
    try:
        with open(prompt_file, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except Exception as e:
        print(f"Warning: Could not read {prompt_file}: {e}", file=sys.stderr)
        return None

def build_training_example_json(system_prompt, tool_definition, user_prompt, widget_code):
    """Build the complete JSON structure that matches create_dataset.py output"""
    # Generate a tool call ID (matching create_dataset.py format)
    tool_call_id = f"call_{uuid.uuid4().hex[:16]}"
    
    # Create the arguments JSON object, then stringify it for the tool call
    arguments_obj = {
        'jsxContent': widget_code
    }
    arguments_json = json.dumps(arguments_obj, ensure_ascii=False)
    
    json_entry = [
        {
            'role': 'system',
            'content': system_prompt,
            'tools': [tool_definition]
        },
        {
            'role': 'user',
            'content': user_prompt
        },
        {
            'role': 'assistant',
            'content': '',
            'tool_calls': [
                {
                    'id': tool_call_id,
                    'type': 'function',
                    'function': {
                        'name': 'WriteUbersichtWidgetToFileSystem',
                        'arguments': arguments_json
                    }
                }
            ]
        }
    ]
    return json.dumps(json_entry, ensure_ascii=False)

def analyze_complete_training_data(csv_file_path, downloads_dir='downloads', prompts_dir='prompts'):
    """
    Analyze complete training examples including all components.
    
    Returns list of results with full size analysis.
    """
    # Load CSV
    widgets = []
    with open(csv_file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row.get('PS_isJSX') == 'Y':
                widgets.append({
                    'id': row['OS_widget_id'],
                    'folder': row['PS_widgetfoldername']
                })
    
    print(f"Analyzing {len(widgets)} complete training examples...\n")
    
    # Calculate static component sizes
    system_prompt_chars = len(SYSTEM_PROMPT)
    tool_def_json = json.dumps(TOOL_DEFINITION, ensure_ascii=False)
    tool_def_chars = len(tool_def_json)
    
    # Build complete JSON to measure structure overhead
    # Note: We use placeholders, but widget code will be JSON-escaped in tool call arguments
    empty_json = build_training_example_json(
        SYSTEM_PROMPT, TOOL_DEFINITION, "USER_PROMPT_PLACEHOLDER", "WIDGET_CODE_PLACEHOLDER"
    )
    # Structure overhead includes: JSON keys, tool call structure, assistant message with content
    # Note: Widget code placeholder doesn't account for JSON escaping overhead, which will be
    # calculated separately when we process actual widget code
    structure_overhead = len(empty_json) - len("USER_PROMPT_PLACEHOLDER") - len("WIDGET_CODE_PLACEHOLDER")
    
    print(f"System prompt: {system_prompt_chars:,} chars (~{estimate_tokens(system_prompt_chars):,} tokens)")
    print(f"Tool definition: {tool_def_chars:,} chars (~{estimate_tokens(tool_def_chars):,} tokens)")
    print(f"JSON structure overhead: {structure_overhead:,} chars (~{estimate_tokens(structure_overhead):,} tokens)")
    print()
    
    # Analyze each training example
    results = []
    for widget in widgets:
        widget_id = widget['id']
        widget_folder = widget['folder']
        
        # Get components
        user_prompt = get_user_prompt(widget_id, prompts_dir)
        widget_code = get_widget_code(widget_folder, downloads_dir)
        
        if user_prompt is None or widget_code is None:
            continue
        
        # Build complete training example
        complete_json = build_training_example_json(
            SYSTEM_PROMPT, TOOL_DEFINITION, user_prompt, widget_code
        )
        
        # Calculate sizes
        total_chars = len(complete_json)
        estimated_tokens = estimate_tokens(total_chars)
        
        # Component breakdown
        user_prompt_chars = len(user_prompt)
        widget_code_chars = len(widget_code)
        
        # Calculate JSON escaping overhead for widget code in tool call arguments
        # Widget code is JSON-stringified in the arguments field
        arguments_obj = {'jsxContent': widget_code}
        arguments_json = json.dumps(arguments_obj, ensure_ascii=False)
        arguments_json_length = len(arguments_json)
        # Escaping overhead = arguments JSON length - raw widget code length - wrapper overhead
        # Wrapper is '{"jsxContent":""}' = 17 chars when empty
        wrapper_overhead = 17
        json_escaping_overhead = arguments_json_length - widget_code_chars - wrapper_overhead
        
        results.append({
            'widget_id': widget_id,
            'widget_folder': widget_folder,
            'system_prompt_tokens': estimate_tokens(system_prompt_chars),
            'tool_def_tokens': estimate_tokens(tool_def_chars),
            'user_prompt_chars': user_prompt_chars,
            'user_prompt_tokens': estimate_tokens(user_prompt_chars),
            'widget_code_chars': widget_code_chars,
            'widget_code_tokens': estimate_tokens(widget_code_chars),
            'json_escaping_overhead_chars': json_escaping_overhead,
            'json_escaping_overhead_tokens': estimate_tokens(json_escaping_overhead),
            'structure_overhead_tokens': estimate_tokens(structure_overhead),
            'total_chars': total_chars,
            'estimated_total_tokens': estimated_tokens,
            'exceeds_limit': estimated_tokens > DEFAULT_MAX_SEQUENCE_LENGTH
        })
    
    return results

def generate_strategy_recommendations(results, max_sequence_length=DEFAULT_MAX_SEQUENCE_LENGTH):
    """
    Generate actionable strategy recommendations for create_dataset.py.
    
    Returns a dictionary mapping widget_id to action:
    - "exclude": Widget should be excluded from training
    - "truncate": Widget should be truncated to fit
    - "keep": Widget fits, keep as-is
    """
    strategy = {}
    
    # Calculate available space for widget code
    if results:
        avg_system_tokens = results[0]['system_prompt_tokens'] + results[0]['tool_def_tokens'] + results[0]['structure_overhead_tokens']
        avg_user_tokens = sum(r['user_prompt_tokens'] for r in results) / len(results)
        available_for_widget_tokens = max_sequence_length - avg_system_tokens - int(avg_user_tokens)
    else:
        available_for_widget_tokens = max_sequence_length
    
    # Determine thresholds for hybrid strategy
    double_limit = max_sequence_length * 2
    
    for r in results:
        widget_id = r['widget_id']
        total_tokens = r['estimated_total_tokens']
        widget_tokens = r['widget_code_tokens']
        
        if total_tokens <= max_sequence_length:
            # Fits within limit
            strategy[widget_id] = {
                'action': 'keep',
                'current_total_tokens': total_tokens,
                'current_widget_tokens': widget_tokens
            }
        else:
            # Exceeds limit - recommend exclusion
            # Note: We exclude rather than truncate because truncation would create
            # incomplete/broken training examples that teach wrong patterns.
            strategy[widget_id] = {
                'action': 'exclude',
                'reason': f'Exceeds limit ({total_tokens} > {max_sequence_length} tokens)',
                'current_total_tokens': total_tokens,
                'current_widget_tokens': widget_tokens,
                'over_by': total_tokens - max_sequence_length
            }
    
    return strategy

def print_analysis(results, max_sequence_length=DEFAULT_MAX_SEQUENCE_LENGTH):
    """Print detailed analysis and recommendations"""
    
    results.sort(key=lambda x: x['estimated_total_tokens'], reverse=True)
    
    total = len(results)
    exceeding = sum(1 for r in results if r['exceeds_limit'])
    within = total - exceeding
    
    print("=" * 100)
    print("TRAINING DATA SIZE ANALYSIS")
    print("=" * 100)
    print()
    print("EVALUATION STRATEGY:")
    print("-" * 100)
    print("This analysis evaluates the training data structure used by create_dataset.py")
    print()
    print("Structure Being Evaluated:")
    print("  [")
    print("    {")
    print("      'role': 'system',")
    print("      'content': systemPrompt (from training_config.py),")
    print("      'tools': [TOOL_DEFINITION (from training_config.py)]")
    print("    },")
    print("    {")
    print("      'role': 'user',")
    print("      'content': user_prompt (from prompts/{widget_id}.prompt)")
    print("    },")
    print("    {")
    print("      'role': 'assistant',")
    print("      'content': '',")
    print("      'tool_calls': [")
    print("        {")
    print("          'id': 'call_...',")
    print("          'type': 'function',")
    print("          'function': {")
    print("            'name': 'WriteUbersichtWidgetToFileSystem',")
    print("            'arguments': '{\"jsxContent\": \"...\"}'  // JSON-stringified widget code")
    print("          }")
    print("        }")
    print("      ]")
    print("    }")
    print("  ]")
    print()
    print("Key Assumptions:")
    print("  ‚Ä¢ System prompt includes tool calling instructions")
    print("  ‚Ä¢ Tool definition is embedded in system message")
    print("  ‚Ä¢ Assistant message contains tool_calls with content set to empty string ('')")
    print("  ‚Ä¢ Tool call arguments contain widget code, JSON-stringified (adds escaping overhead)")
    print("  ‚Ä¢ When tool_calls are present, content is empty (tool_calls are the primary response)")
    print("  ‚Ä¢ Matches create_dataset.py output format")
    print()
    print("‚ö†Ô∏è  Note: This structure is assumed/verified against create_dataset.py")
    print("   If you're evaluating alternative structures, modify this script accordingly.")
    print()
    print("=" * 100)
    print()
    print(f"\nTotal training examples: {total}")
    print(f"Max sequence length limit: {max_sequence_length} tokens")
    print(f"\nExamples exceeding limit: {exceeding} ({exceeding/total*100:.1f}%)")
    print(f"Examples within limit: {within} ({within/total*100:.1f}%)")
    
    # Statistics
    total_tokens = [r['estimated_total_tokens'] for r in results]
    widget_tokens = [r['widget_code_tokens'] for r in results]
    
    if total_tokens:
        print(f"\n{'='*100}")
        print("TOKEN STATISTICS")
        print("=" * 100)
        print(f"{'Metric':<40} {'Min':<12} {'Max':<12} {'Avg':<12} {'Median':<12}")
        print("-" * 100)
        print(f"{'Total tokens (full example)':<40} {min(total_tokens):<12} {max(total_tokens):<12} {sum(total_tokens)/len(total_tokens):<12.0f} {sorted(total_tokens)[len(total_tokens)//2]:<12}")
        print(f"{'Widget code tokens only':<40} {min(widget_tokens):<12} {max(widget_tokens):<12} {sum(widget_tokens)/len(widget_tokens):<12.0f} {sorted(widget_tokens)[len(widget_tokens)//2]:<12}")
    
    # Show component breakdown for average example
    if results:
        avg_result = results[len(results)//2]  # Median example
        print(f"\n{'='*100}")
        print("COMPONENT BREAKDOWN (Median Example)")
        print("=" * 100)
        print(f"System prompt:        {avg_result['system_prompt_tokens']:>6} tokens")
        print(f"Tool definition:      {avg_result['tool_def_tokens']:>6} tokens")
        print(f"User prompt:          {avg_result['user_prompt_tokens']:>6} tokens")
        print(f"Widget code (raw):    {avg_result['widget_code_tokens']:>6} tokens")
        print(f"JSON escaping overhead: {avg_result['json_escaping_overhead_tokens']:>6} tokens")
        print(f"JSON structure:       {avg_result['structure_overhead_tokens']:>6} tokens")
        print(f"{'-'*40}")
        print(f"TOTAL:                {avg_result['estimated_total_tokens']:>6} tokens")
    
    # Top 10 largest
    print(f"\n{'='*100}")
    print("TOP 10 LARGEST TRAINING EXAMPLES")
    print("=" * 100)
    print(f"{'Widget ID':<30} {'Total':<12} {'Widget':<12} {'User':<12} {'Status'}")
    print("-" * 100)
    
    for r in results[:10]:
        status = "‚ö†Ô∏è EXCEEDS" if r['exceeds_limit'] else "‚úì OK"
        print(f"{r['widget_id']:<30} {r['estimated_total_tokens']:<12} {r['widget_code_tokens']:<12} {r['user_prompt_tokens']:<12} {status}")
    
    # Widgets exceeding limit
    if exceeding > 0:
        print(f"\n{'='*100}")
        print(f"EXAMPLES EXCEEDING {max_sequence_length} TOKEN LIMIT")
        print("=" * 100)
        print(f"{'Widget ID':<30} {'Total':<12} {'Widget':<12} {'Over by':<12}")
        print("-" * 100)
        
        for r in results:
            if r['exceeds_limit']:
                over_by = r['estimated_total_tokens'] - max_sequence_length
                print(f"{r['widget_id']:<30} {r['estimated_total_tokens']:<12} {r['widget_code_tokens']:<12} {over_by:<12}")
    
    # Recommendations
    print(f"\n{'='*100}")
    print("RECOMMENDATIONS")
    print("=" * 100)
    
    if exceeding == 0:
        print("\n‚úì All training examples fit within the limit. No chunking needed!")
    else:
        # Calculate how much widget code space is available
        avg_system_tokens = results[0]['system_prompt_tokens'] + results[0]['tool_def_tokens'] + results[0]['structure_overhead_tokens']
        avg_user_tokens = sum(r['user_prompt_tokens'] for r in results) / len(results)
        available_for_widget = max_sequence_length - avg_system_tokens - int(avg_user_tokens)
        
        print(f"\n‚ö†Ô∏è {exceeding} examples exceed the limit.")
        print(f"   Available tokens for widget code (after system/user/structure): ~{available_for_widget}")
        print(f"   Average widget code tokens: {sum(widget_tokens)/len(widget_tokens):.0f}")
        print()
        
        print("STRATEGIES:\n")
        
        print("RECOMMENDED STRATEGY: EXCLUSION")
        print(f"   - Remove {exceeding} examples that exceed the limit")
        print(f"   - Keep {within} examples that fit within limit")
        print(f"   - Data loss: {exceeding/total*100:.1f}%")
        print(f"\n   Rationale: Truncation would create incomplete/broken training examples")
        print(f"   that teach wrong patterns. Better to exclude and maintain data quality.\n")
    
    # Generate actionable strategy recommendations
    strategy = generate_strategy_recommendations(results, max_sequence_length)
    
    # Count actions
    exclude_count = sum(1 for s in strategy.values() if s['action'] == 'exclude')
    keep_count = sum(1 for s in strategy.values() if s['action'] == 'keep')
    
    print(f"\n{'='*100}")
    print("RECOMMENDED STRATEGY FOR create_dataset.py")
    print("=" * 100)
    print(f"\nAction Summary:")
    print(f"  ‚Ä¢ Keep as-is: {keep_count} widgets")
    print(f"  ‚Ä¢ Exclude: {exclude_count} widgets (exceed token limit)")
    print()
    
    if exclude_count > 0:
        print("Widgets to EXCLUDE:")
        print(f"{'Widget ID':<30} {'Total Tokens':<12} {'Over by':<12} {'Reason'}")
        print("-" * 100)
        for widget_id, action_data in strategy.items():
            if action_data['action'] == 'exclude':
                print(f"{widget_id:<30} {action_data['current_total_tokens']:<12} {action_data['over_by']:<12} {action_data['reason']}")
        print()
    
    # Export results
    output_file = 'training_data_size_analysis.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'max_sequence_length': max_sequence_length,
            'total_examples': total,
            'exceeding_limit': exceeding,
            'within_limit': within,
            'examples': results
        }, f, indent=2)
    
    # Export strategy file for create_dataset.py
    strategy_file = 'training_data_strategy.json'
    with open(strategy_file, 'w', encoding='utf-8') as f:
        json.dump({
            'max_sequence_length': max_sequence_length,
            'strategy': strategy,
            'summary': {
                'keep': keep_count,
                'exclude': exclude_count
            }
        }, f, indent=2)
    
    print(f"‚úì Detailed analysis saved to: {output_file}")
    print(f"‚úì Strategy recommendations saved to: {strategy_file}")
    print()
    print(f"üìã Next step: Use {strategy_file} to implement strategy in create_dataset.py")
    print(f"   The strategy file contains specific actions for each widget:")
    print(f"   - 'keep': Include widget as-is")
    print(f"   - 'exclude': Skip this widget entirely (exceeds token limit)")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Evaluate complete training data size including system prompts, user prompts, and widget code. Generates actionable strategy recommendations for create_dataset.py.'
    )
    parser.add_argument('--csv', default='widget_processing_results.csv',
                       help='Path to widget_processing_results.csv')
    parser.add_argument('--downloads', default='downloads',
                       help='Directory containing widget files')
    parser.add_argument('--prompts', default='prompts',
                       help='Directory containing prompt files')
    parser.add_argument('--max-tokens', type=int, default=DEFAULT_MAX_SEQUENCE_LENGTH,
                       help='Maximum sequence length in tokens (default: 4095)')
    
    args = parser.parse_args()
    
    results = analyze_complete_training_data(args.csv, args.downloads, args.prompts)
    print_analysis(results, args.max_tokens)

if __name__ == '__main__':
    main()

